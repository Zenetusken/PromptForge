<project_specification>
  <project_name>PromptForge - Intelligent Prompt Optimization Engine</project_name>

  <overview>
    PromptForge is a self-adaptive prompt optimization webapp that transforms raw, unstructured prompts
    into properly structured, improved, and optimized versions. The system autonomously analyzes incoming
    prompts, detects their type and weaknesses, selects the optimal optimization strategy, applies it,
    and validates the result — all without human configuration. It leverages the Claude Agent SDK
    (claude-agent-sdk) to invoke Claude via the existing Max subscription through the CLI, ensuring
    zero API costs. The UI features a cyberpunk/hacker aesthetic with real-time streaming of each
    pipeline step, a toggleable diff view, and persistent optimization history.
  </overview>

  <technology_stack>
    <frontend>
      <framework>SvelteKit 2 (Svelte 5) with Vite 7.3+</framework>
      <styling>Tailwind CSS 4 with custom cyberpunk theme</styling>
      <state_management>Svelte 5 runes ($state, $derived, $effect)</state_management>
      <routing>SvelteKit file-based routing</routing>
      <diff_engine>diff npm package for text comparison</diff_engine>
      <syntax_highlighting>highlight.js or Prism for code blocks in prompts</syntax_highlighting>
      <fonts>JetBrains Mono (monospace primary), Inter (UI secondary)</fonts>
      <port>5173 (dev) / 4173 (preview)</port>
    </frontend>
    <backend>
      <runtime>Python 3.14+ with FastAPI</runtime>
      <llm_integration>claude-agent-sdk (async, streams via Claude Code CLI)</llm_integration>
      <database>SQLite via aiosqlite + SQLAlchemy async</database>
      <streaming>Server-Sent Events (SSE) via FastAPI StreamingResponse</streaming>
      <task_queue>asyncio tasks for pipeline orchestration</task_queue>
      <port>8000</port>
    </backend>
    <communication>
      <api>RESTful endpoints with OpenAPI auto-docs</api>
      <streaming>SSE for real-time pipeline step streaming</streaming>
      <serialization>Pydantic v2 models for request/response validation</serialization>
    </communication>
    <deployment>
      <containerization>Docker with docker-compose (multi-stage builds)</containerization>
      <frontend_build>SvelteKit adapter-node for production, served behind FastAPI or standalone</frontend_build>
      <database_volume>Docker volume for SQLite persistence</database_volume>
    </deployment>
  </technology_stack>

  <prerequisites>
    <environment_setup>
      - Claude Code CLI installed and authenticated with Max subscription ($200/mo)
      - Node.js 22+ (required by claude-agent-sdk internally and Vite 7.3+)
      - Python 3.14+
      - Docker and docker-compose
      - ANTHROPIC_API_KEY must NOT be set (to use subscription, not pay-per-token)
    </environment_setup>
    <dependencies>
      <python>
        - claude-agent-sdk (latest, replaces deprecated claude-code-sdk)
        - fastapi
        - uvicorn[standard]
        - aiosqlite
        - sqlalchemy[asyncio]
        - pydantic>=2.0
        - python-dotenv
        - anyio
        - mcp (MCP Python SDK — provides FastMCP for the MCP server)
      </python>
      <node>
        - @sveltejs/kit
        - svelte (5.x, latest)
        - @tailwindcss/vite (4.x, latest — Vite plugin, replaces PostCSS plugin)
        - tailwindcss (4.x, latest)
        - diff (text diffing library, latest)
        - highlight.js (latest)
        - vite (>=7.3)
      </node>
    </dependencies>
  </prerequisites>

  <core_features>
    <optimization_pipeline>
      The pipeline is the heart of PromptForge. It operates as a 3-step autonomous process,
      where each step uses claude-agent-sdk to call Claude via the CLI (free with subscription).
      All steps stream their output in real-time to the frontend via SSE.

      <step_1_analyze>
        PURPOSE: Classify the prompt and identify optimization opportunities.

        INPUT: Raw user prompt (string)
        OUTPUT (structured JSON):
          - task_type: string (one of: "coding", "creative_writing", "analytical", "conversational",
            "extraction", "instruction", "research", "brainstorming", "debugging", "system_prompt")
          - weaknesses: list[string] — identified issues (ambiguity, vague language, missing context,
            no output format, no constraints, lacks role/persona, missing examples, no success criteria)
          - strengths: list[string] — what the prompt already does well
          - complexity: string ("simple", "moderate", "complex")
          - recommended_frameworks: list[string] — which optimization frameworks to apply
            (selected from: "CO-STAR", "RISEN", "chain-of-thought", "few-shot-scaffolding",
            "role-task-format", "structured-output", "step-by-step", "constraint-injection",
            "context-enrichment", "persona-assignment")

        SYSTEM PROMPT STRATEGY: The analysis step uses a carefully crafted system prompt that
        instructs Claude to act as a prompt engineering expert. It should evaluate the raw prompt
        against a comprehensive rubric covering:
          1. Clarity — Is the intent unambiguous?
          2. Specificity — Are requirements concrete and measurable?
          3. Structure — Is there logical organization?
          4. Context — Is sufficient background provided?
          5. Constraints — Are boundaries and limitations defined?
          6. Output Format — Is the expected response format specified?
          7. Examples — Are illustrative examples included where helpful?
          8. Persona/Role — Is there a useful role assignment?

        CLAUDE-AGENT-SDK USAGE:
          - Use ClaudeAgentOptions with:
            - system_prompt: The analysis expert prompt
            - permission_mode: "bypass_permissions"
            - max_turns: 1
            - output_format: JSON schema for structured output
          - Use the query() function (stateless, single-shot)
          - Parse the structured JSON response
      </step_1_analyze>

      <step_2_optimize>
        PURPOSE: Transform the raw prompt using the strategies identified in Step 1.

        INPUT: Raw prompt + analysis results from Step 1
        OUTPUT (structured JSON):
          - optimized_prompt: string — the improved, restructured prompt
          - changes_made: list[string] — specific improvements applied
          - framework_applied: string — primary framework used
          - optimization_notes: string — brief explanation of approach

        STRATEGY SELECTION (autonomous, based on Step 1 analysis):
          The system prompt for this step is dynamically constructed based on the analysis results.
          It includes:

          IF task_type == "coding":
            Apply: structured-output + constraint-injection + step-by-step
            Add: language specification, input/output examples, edge cases, error handling requirements

          IF task_type == "creative_writing":
            Apply: persona-assignment + context-enrichment + CO-STAR
            Add: tone, audience, style constraints, length requirements

          IF task_type == "analytical":
            Apply: chain-of-thought + structured-output + CO-STAR
            Add: data context, analysis framework, output format specification

          IF task_type == "conversational":
            Apply: persona-assignment + context-enrichment + role-task-format
            Add: conversation context, tone guidelines, response format

          IF task_type == "extraction":
            Apply: structured-output + constraint-injection + few-shot-scaffolding
            Add: source format, target schema, edge case handling

          IF task_type == "instruction":
            Apply: step-by-step + constraint-injection + role-task-format
            Add: prerequisites, success criteria, verification steps

          IF task_type == "research":
            Apply: chain-of-thought + CO-STAR + structured-output
            Add: scope boundaries, source requirements, depth specification

          IF task_type == "brainstorming":
            Apply: persona-assignment + context-enrichment + RISEN
            Add: quantity targets, diversity requirements, evaluation criteria

          IF task_type == "debugging":
            Apply: chain-of-thought + step-by-step + constraint-injection
            Add: environment details, reproduction steps, expected vs actual behavior

          IF task_type == "system_prompt":
            Apply: role-task-format + constraint-injection + persona-assignment
            Add: behavioral boundaries, output format rules, example interactions

          FRAMEWORK DEFINITIONS (injected into system prompt):

          CO-STAR: Context, Objective, Style, Tone, Audience, Response format
          RISEN: Role, Instructions, Steps, End goal, Narrowing (constraints)
          Chain-of-Thought: "Think step by step" scaffolding with intermediate reasoning
          Few-Shot Scaffolding: Add 1-2 input/output examples to demonstrate expected format
          Role-Task-Format: Clear role assignment + task description + output format
          Structured Output: JSON/markdown/table output format specification
          Step-by-Step: Numbered sequential instructions
          Constraint Injection: Explicit boundaries (do/don't, length, format, scope)
          Context Enrichment: Background information, domain context, prior knowledge
          Persona Assignment: Expert role with relevant domain expertise

        CLAUDE-AGENT-SDK USAGE:
          - Use ClaudeAgentOptions with:
            - system_prompt: Dynamically constructed based on analysis (see above)
            - permission_mode: "bypass_permissions"
            - max_turns: 1
            - output_format: JSON schema for structured output
            - include_partial_messages: True (for streaming)
          - Stream partial messages to frontend via SSE as they arrive
      </step_2_optimize>

      <step_3_validate>
        PURPOSE: Verify the optimized prompt is genuinely better than the original.

        INPUT: Original prompt + optimized prompt + changes_made
        OUTPUT (structured JSON):
          - is_improvement: boolean — overall verdict
          - clarity_score: integer (1-10)
          - specificity_score: integer (1-10)
          - structure_score: integer (1-10)
          - faithfulness_score: integer (1-10) — does it preserve the original intent?
          - overall_score: integer (1-10)
          - issues: list[string] — any problems found (empty if none)
          - verdict: string — brief human-readable summary

        SYSTEM PROMPT: Act as a critical prompt quality assessor. Compare the original and
        optimized versions across the scoring dimensions. Be strict — only mark as improvement
        if genuinely better. Flag if the optimization changed the user's intent, added
        unnecessary complexity, or lost important nuance.

        CLAUDE-AGENT-SDK USAGE:
          - Same pattern as Steps 1 and 2
          - If is_improvement is false and overall_score < 5, consider re-running Step 2
            with adjusted constraints (max 1 retry to prevent loops)
      </step_3_validate>
    </optimization_pipeline>

    <streaming_ux>
      Each pipeline step streams its progress to the frontend via SSE:

      SSE Event Types:
        - "step_start": { step: "analyze"|"optimize"|"validate", step_number: 1|2|3 }
        - "step_progress": { step: string, content: string (partial text/JSON) }
        - "step_complete": { step: string, result: object (full structured result) }
        - "pipeline_complete": { optimization_id: string, total_duration_ms: number }
        - "pipeline_error": { step: string, error: string, recoverable: boolean }

      The frontend displays each step as an expandable card that:
        1. Shows a pulsing neon indicator while the step is in progress
        2. Streams incoming text content in real-time
        3. Collapses with a summary checkmark when complete
        4. Auto-expands the final result (optimized prompt + diff view)
    </streaming_ux>

    <diff_view>
      Two display modes, togglable via a switch:

      <side_by_side>
        - Left panel: Original prompt (with line numbers)
        - Right panel: Optimized prompt (with line numbers)
        - Synchronized scrolling
        - Highlighted additions (neon green) and removals (neon red)
      </side_by_side>

      <inline_diff>
        - Single panel with unified diff
        - Additions highlighted with green background
        - Removals highlighted with red background and strikethrough
        - Line-level and word-level diff granularity toggle
      </inline_diff>

      Implementation: Use the `diff` npm package to compute changes client-side.
      Display with custom Svelte components using scoped CSS.
    </diff_view>

    <history>
      SQLite-backed persistent history of all optimization runs.

      <features>
        - Chronological list of all optimizations with timestamps
        - Search/filter by original prompt text
        - Click to re-view any past optimization (analysis, result, diff, scores)
        - Delete individual entries
        - Re-optimize: take a past prompt and run it through the pipeline again
        - Export: copy optimized prompt to clipboard, or download as .txt/.md
      </features>

      <database_schema>
        TABLE optimizations:
          - id: TEXT PRIMARY KEY (UUID)
          - created_at: DATETIME DEFAULT CURRENT_TIMESTAMP
          - raw_prompt: TEXT NOT NULL
          - optimized_prompt: TEXT
          - task_type: TEXT
          - complexity: TEXT
          - weaknesses: TEXT (JSON array)
          - strengths: TEXT (JSON array)
          - changes_made: TEXT (JSON array)
          - framework_applied: TEXT
          - optimization_notes: TEXT
          - clarity_score: INTEGER
          - specificity_score: INTEGER
          - structure_score: INTEGER
          - faithfulness_score: INTEGER
          - overall_score: INTEGER
          - is_improvement: BOOLEAN
          - verdict: TEXT
          - duration_ms: INTEGER
          - model_used: TEXT
          - status: TEXT DEFAULT 'completed' (pending, running, completed, failed)
          - error_message: TEXT
          - project: TEXT DEFAULT NULL (optional project/collection name for grouping)
          - tags: TEXT DEFAULT '[]' (JSON array of string tags for flexible categorization)
          - title: TEXT DEFAULT NULL (optional human-readable title, auto-generated if not provided)

        INDEX idx_optimizations_project ON optimizations(project)
        INDEX idx_optimizations_task_type ON optimizations(task_type)
        INDEX idx_optimizations_created_at ON optimizations(created_at DESC)
      </database_schema>
    </history>

    <clipboard_integration>
      - One-click copy of optimized prompt to clipboard
      - Visual feedback (neon flash animation on copy)
      - Also copy original, diff, or analysis separately
    </clipboard_integration>

    <mcp_server>
      PromptForge exposes its optimization engine and prompt history as an MCP (Model Context
      Protocol) server, allowing Claude Code and any MCP-compatible client to optimize prompts
      and retrieve past optimizations directly from the terminal / IDE without opening the web UI.

      <server_config>
        NAME: promptforge_mcp
        TRANSPORT: stdio (local tool, runs as subprocess of Claude Code)
        FRAMEWORK: FastMCP (from mcp Python SDK)
        DATABASE: Shares the same SQLite database as the FastAPI backend
        SERVICES: Reuses the same pipeline services (analyzer, optimizer, validator, claude_client)
      </server_config>

      <tools>
        <tool name="promptforge_optimize">
          DESCRIPTION: Run the full optimization pipeline on a raw prompt. Analyzes weaknesses,
          selects the best optimization strategy, transforms the prompt, and validates the result.
          Returns the optimized prompt with scores and analysis.

          INPUT (Pydantic model):
            - prompt: str (required) — The raw prompt to optimize
            - project: Optional[str] — Project name to file the result under
            - tags: Optional[list[str]] — Tags for categorization
            - title: Optional[str] — Human-readable title (auto-generated if omitted)

          OUTPUT (JSON):
            {
              "id": "uuid",
              "optimized_prompt": "the improved prompt text",
              "task_type": "coding",
              "changes_made": ["Added output format specification", "Injected step-by-step structure"],
              "framework_applied": "chain-of-thought + structured-output",
              "scores": {
                "clarity": 9,
                "specificity": 8,
                "structure": 9,
                "faithfulness": 10,
                "overall": 9
              },
              "verdict": "Significant improvement: added clear structure and output format."
            }

          ANNOTATIONS:
            readOnlyHint: false
            destructiveHint: false
            idempotentHint: false (each run may produce different results)
            openWorldHint: false
        </tool>

        <tool name="promptforge_get">
          DESCRIPTION: Retrieve a specific past optimization by its ID. Returns the full
          optimization record including original prompt, optimized prompt, analysis, scores,
          and metadata.

          INPUT:
            - optimization_id: str (required) — UUID of the optimization

          OUTPUT (JSON): Full optimization record

          ANNOTATIONS:
            readOnlyHint: true
            destructiveHint: false
            idempotentHint: true
            openWorldHint: false
        </tool>

        <tool name="promptforge_list">
          DESCRIPTION: List past optimizations with filtering, search, and pagination.
          Supports filtering by project, tag, task type, and minimum score.

          INPUT (Pydantic model):
            - project: Optional[str] — Filter by project name
            - tag: Optional[str] — Filter by tag (matches any optimization containing this tag)
            - task_type: Optional[str] — Filter by task type (coding, creative_writing, etc.)
            - min_score: Optional[int] — Filter by minimum overall score (1-10)
            - search: Optional[str] — Full-text search across prompt content
            - limit: Optional[int] — Max results (default 20, max 100)
            - offset: Optional[int] — Pagination offset (default 0)
            - sort: Optional[str] — Sort field: "created_at", "overall_score", "task_type" (default: "created_at")
            - order: Optional[str] — "asc" or "desc" (default: "desc")

          OUTPUT (JSON):
            {
              "total": 42,
              "count": 20,
              "offset": 0,
              "has_more": true,
              "next_offset": 20,
              "items": [
                {
                  "id": "uuid",
                  "title": "Auto-generated or user title",
                  "raw_prompt_preview": "First 100 chars...",
                  "task_type": "coding",
                  "overall_score": 9,
                  "project": "my-cli-tool",
                  "tags": ["python", "refactoring"],
                  "created_at": "2026-02-12T10:30:00Z"
                }
              ]
            }

          ANNOTATIONS:
            readOnlyHint: true
            destructiveHint: false
            idempotentHint: true
            openWorldHint: false
        </tool>

        <tool name="promptforge_get_by_project">
          DESCRIPTION: Get all optimized prompts for a specific project, sorted by most recent.
          Convenience tool for quickly pulling all prompts associated with a project.

          INPUT:
            - project: str (required) — Project name
            - include_prompts: Optional[bool] — Include full prompt text (default: true)
            - limit: Optional[int] — Max results (default 50)

          OUTPUT (JSON): List of optimization records for the project

          ANNOTATIONS:
            readOnlyHint: true
            destructiveHint: false
            idempotentHint: true
            openWorldHint: false
        </tool>

        <tool name="promptforge_search">
          DESCRIPTION: Full-text search across all optimized prompts. Searches both the
          original and optimized prompt content, titles, tags, and project names.

          INPUT:
            - query: str (required) — Search query (min 2 chars)
            - limit: Optional[int] — Max results (default 20)

          OUTPUT (JSON): Matching optimizations with relevance-sorted results

          ANNOTATIONS:
            readOnlyHint: true
            destructiveHint: false
            idempotentHint: true
            openWorldHint: false
        </tool>

        <tool name="promptforge_tag">
          DESCRIPTION: Add or remove tags from an existing optimization. Also supports
          setting/changing the project and title.

          INPUT:
            - optimization_id: str (required) — UUID of the optimization
            - add_tags: Optional[list[str]] — Tags to add
            - remove_tags: Optional[list[str]] — Tags to remove
            - project: Optional[str] — Set or change project name (use "" to clear)
            - title: Optional[str] — Set or change title

          OUTPUT (JSON): Updated optimization metadata

          ANNOTATIONS:
            readOnlyHint: false
            destructiveHint: false
            idempotentHint: true
            openWorldHint: false
        </tool>

        <tool name="promptforge_stats">
          DESCRIPTION: Get usage statistics and overview of the prompt library.
          Shows totals, averages, breakdowns by project and task type.

          INPUT:
            - project: Optional[str] — Scope stats to a specific project

          OUTPUT (JSON):
            {
              "total_optimizations": 142,
              "avg_overall_score": 7.8,
              "projects": {"my-cli-tool": 23, "blog-posts": 15, ...},
              "task_types": {"coding": 45, "creative_writing": 30, ...},
              "top_frameworks": {"chain-of-thought": 38, "CO-STAR": 25, ...},
              "optimizations_today": 5,
              "optimizations_this_week": 22
            }

          ANNOTATIONS:
            readOnlyHint: true
            destructiveHint: false
            idempotentHint: true
            openWorldHint: false
        </tool>

        <tool name="promptforge_delete">
          DESCRIPTION: Delete a specific optimization from the library.

          INPUT:
            - optimization_id: str (required) — UUID of the optimization

          OUTPUT (JSON): { "deleted": true, "id": "uuid" }

          ANNOTATIONS:
            readOnlyHint: false
            destructiveHint: true
            idempotentHint: true
            openWorldHint: false
        </tool>
      </tools>

      <claude_code_integration>
        The MCP server is configured in the user's Claude Code settings so it's available
        in every Claude Code session.

        SETUP (added to ~/.claude/claude_code_config.json or project .mcp.json):
        {
          "mcpServers": {
            "promptforge": {
              "command": "python",
              "args": ["-m", "app.mcp_server"],
              "cwd": "/path/to/promptforge/backend",
              "env": {
                "PROMPTFORGE_DB_PATH": "/path/to/promptforge/data/promptforge.db"
              }
            }
          }
        }

        USAGE EXAMPLES FROM CLAUDE CODE:
          - "Optimize this prompt: [paste prompt]"
            → Claude calls promptforge_optimize, returns improved version

          - "Show me all prompts for the 'my-api' project"
            → Claude calls promptforge_get_by_project(project="my-api")

          - "Find my prompt about database migrations"
            → Claude calls promptforge_search(query="database migrations")

          - "Tag that last optimization with 'python' and 'refactoring'"
            → Claude calls promptforge_tag(optimization_id=..., add_tags=["python","refactoring"])

          - "What are my prompt stats this week?"
            → Claude calls promptforge_stats()
      </claude_code_integration>
    </mcp_server>
  </core_features>

  <api_endpoints>
    <optimization>
      POST /api/optimize
        Request: { "prompt": string }
        Response: SSE stream (see streaming_ux event types above)
        Description: Starts the full optimization pipeline. Returns an SSE stream
        with real-time updates for each step. The optimization is also persisted to SQLite.

      GET /api/optimize/{id}
        Response: Full optimization record (JSON)
        Description: Retrieve a specific past optimization by ID.

      POST /api/optimize/{id}/retry
        Response: SSE stream (new optimization run)
        Description: Re-run optimization on a previously submitted prompt.
    </optimization>

    <history>
      GET /api/history
        Query params: ?page=1&per_page=20&search=<text>&sort=created_at&order=desc
        Response: { items: Optimization[], total: int, page: int, per_page: int }
        Description: Paginated list of past optimizations.

      DELETE /api/history/{id}
        Response: { success: boolean }
        Description: Delete a specific optimization record.

      GET /api/history/stats
        Response: { total_optimizations: int, avg_score: float, most_common_type: string,
                    optimizations_today: int }
        Description: Dashboard statistics.
    </history>

    <health>
      GET /api/health
        Response: { status: "ok", claude_available: boolean, db_connected: boolean,
                    version: string }
        Description: Health check endpoint.
    </health>
  </api_endpoints>

  <ui_layout>
    <overall_structure>
      Single-page application with a full-viewport cyberpunk-themed layout.

      ┌─────────────────────────────────────────────────────────────┐
      │  HEADER: Logo + "PromptForge" + Stats Badge + Theme Toggle │
      ├────────────┬──────────────────────────────────────────────── │
      │            │                                                │
      │  SIDEBAR   │              MAIN AREA                         │
      │  History   │                                                │
      │  List      │  ┌──────────────────────────────────────────┐  │
      │            │  │  PROMPT INPUT                            │  │
      │  - Entry 1 │  │  [Large textarea with neon border]       │  │
      │  - Entry 2 │  │  [Forge It! button]                      │  │
      │  - Entry 3 │  │                                          │  │
      │  ...       │  └──────────────────────────────────────────┘  │
      │            │                                                │
      │            │  ┌──────────────────────────────────────────┐  │
      │            │  │  PIPELINE PROGRESS                       │  │
      │            │  │  [Step 1: Analyzing...  ◉ streaming]     │  │
      │            │  │  [Step 2: Optimizing... ○ pending ]      │  │
      │            │  │  [Step 3: Validating... ○ pending ]      │  │
      │            │  └──────────────────────────────────────────┘  │
      │            │                                                │
      │            │  ┌──────────────────────────────────────────┐  │
      │            │  │  RESULT AREA                             │  │
      │            │  │  [Diff View: Side-by-side | Inline]      │  │
      │            │  │  [Scores: ██████░░ 7.8/10]               │  │
      │            │  │  [Copy] [Export] [Re-optimize]           │  │
      │            │  └──────────────────────────────────────────┘  │
      │            │                                                │
      ├────────────┴────────────────────────────────────────────────┤
      │  FOOTER: Version + Pipeline Status                         │
      └─────────────────────────────────────────────────────────────┘
    </overall_structure>

    <sidebar>
      - Collapsible (toggle via hamburger icon)
      - Shows history list with:
        - First ~50 chars of raw prompt as title
        - Timestamp (relative: "2h ago", "yesterday")
        - Overall score badge (color-coded: green 8+, yellow 5-7, red <5)
        - Task type icon/badge
      - Search bar at top with neon glow on focus
      - "Clear History" button at bottom (with confirmation)
      - Responsive: hidden by default on mobile, overlay on toggle
    </sidebar>

    <main_area>
      <prompt_input>
        - Large, resizable textarea (min 4 lines, max 20 lines)
        - Monospace font (JetBrains Mono)
        - Neon cyan border that pulses on focus
        - Placeholder text: "Paste your prompt here... PromptForge will handle the rest."
        - Character count in bottom-right corner
        - "Forge It!" button below:
          - Neon gradient (cyan to purple) background
          - Disabled state while pipeline is running
          - Loading animation (forging/hammering icon) during processing
        - Keyboard shortcut: Ctrl+Enter to submit
      </prompt_input>

      <pipeline_progress>
        Three collapsible step cards, stacked vertically:

        Each card shows:
          - Step number + name (e.g., "01 // ANALYZE")
          - Status indicator: spinning neon ring (active), checkmark (done), circle (pending)
          - Duration timer while active
          - Expandable content area:
            - While streaming: raw text appearing character by character
            - When complete: formatted result (analysis details, etc.)
          - Cards auto-collapse when the next step starts
          - Final step auto-expands its result

        Step-specific content:
          - Step 1 (Analyze): Shows task_type badge, weakness list, strength list
          - Step 2 (Optimize): Streams the optimized prompt text as it generates
          - Step 3 (Validate): Shows score bars and verdict
      </pipeline_progress>

      <result_area>
        Appears after pipeline completes. Contains:

        <diff_view_panel>
          - Toggle switch: "Side-by-side" | "Inline"
          - The diff comparison (see diff_view feature above)
          - Monospace font, line numbers, syntax-aware highlighting
        </diff_view_panel>

        <score_panel>
          - Horizontal bar chart showing all 5 scores (clarity, specificity, structure,
            faithfulness, overall)
          - Neon-colored bars (cyan for high scores, yellow for medium, red for low)
          - Verdict text below
        </score_panel>

        <action_buttons>
          - [Copy Optimized] — copies to clipboard with flash animation
          - [Export .md] — downloads as markdown file
          - [Re-forge] — re-runs the pipeline on the same prompt
          - [Edit & Re-forge] — loads the optimized prompt back into the input for iteration
        </action_buttons>

        <changes_summary>
          - Expandable list of changes_made from Step 2
          - Each change is a bullet point with a neon green "+" prefix
          - Framework applied shown as a badge
        </changes_summary>
      </result_area>
    </main_area>
  </ui_layout>

  <design_system>
    <theme>Cyberpunk / Hacker</theme>

    <color_palette>
      <!-- Backgrounds -->
      bg-primary: #0a0a0f (near-black with blue tint)
      bg-secondary: #12121a (slightly lighter)
      bg-card: #1a1a2e (card backgrounds)
      bg-input: #0f0f1a (input fields)
      bg-hover: #1e1e35 (hover states)

      <!-- Neon Accents -->
      neon-cyan: #00f0ff (primary accent — buttons, active states, borders)
      neon-purple: #b000ff (secondary accent — gradients, highlights)
      neon-green: #00ff88 (success, additions in diff, high scores)
      neon-red: #ff0055 (errors, removals in diff, low scores)
      neon-yellow: #ffee00 (warnings, medium scores)
      neon-orange: #ff6600 (active/processing states)

      <!-- Text -->
      text-primary: #e0e0f0 (main text — slight blue tint)
      text-secondary: #8888aa (muted text)
      text-dim: #555577 (very muted — timestamps, metadata)

      <!-- Glow effects -->
      glow-cyan: 0 0 10px #00f0ff, 0 0 20px #00f0ff40
      glow-purple: 0 0 10px #b000ff, 0 0 20px #b000ff40
      glow-green: 0 0 10px #00ff88, 0 0 20px #00ff8840
    </color_palette>

    <typography>
      <primary_font>JetBrains Mono — all prompt text, code, pipeline output</primary_font>
      <secondary_font>Inter — UI labels, buttons, navigation, metadata</secondary_font>
      <heading_style>
        Uppercase, letter-spacing: 0.15em, font-weight: 700
        Example: "01 // ANALYZE" in small caps with neon glow
      </heading_style>
      <body_text>14px base, 1.6 line-height for prompt text</body_text>
      <mono_text>13px for code/prompt display, 1.5 line-height</mono_text>
    </typography>

    <component_styles>
      <buttons>
        - Primary: neon-cyan to neon-purple gradient, rounded-lg, subtle glow on hover
        - Secondary: transparent with neon-cyan border, glow on hover
        - Danger: transparent with neon-red border
        - All buttons: transition-all 200ms, slight scale(1.02) on hover
        - Disabled: opacity 0.4, no glow, cursor-not-allowed
      </buttons>

      <cards>
        - bg-card background with 1px border (neon-cyan at 20% opacity)
        - Rounded-xl corners
        - Subtle glow on hover/active
        - Pipeline step cards have left border accent (colored by status)
      </cards>

      <inputs>
        - bg-input background
        - 1px border neon-cyan at 30% opacity, full glow on focus
        - Monospace font
        - Placeholder text in text-dim color
      </inputs>

      <badges>
        - Small rounded-full pills
        - Semi-transparent background with matching border
        - Used for: task_type, framework_applied, scores, status
      </badges>

      <scrollbar>
        - Thin (6px), rounded
        - Track: bg-secondary
        - Thumb: neon-cyan at 40% opacity, full opacity on hover
      </scrollbar>
    </component_styles>

    <animations>
      - Neon pulse: keyframe animation for active step indicators (glow oscillation)
      - Text stream: characters appear with slight fade-in (opacity 0 -> 1 over 50ms)
      - Card expand/collapse: smooth height transition (300ms ease-out)
      - Score bars: animate width from 0 to final value (800ms ease-out, staggered)
      - Copy flash: brief neon-green flash overlay on the copied element
      - Loading/forging: custom SVG animation (anvil + sparks, or pulsing hexagon)
      - Sidebar slide: translateX transition (250ms)
      - Page transitions: fade (150ms)
    </animations>
  </design_system>

  <key_interactions>
    <optimization_flow>
      1. User pastes/types prompt into the input textarea
      2. User clicks "Forge It!" or presses Ctrl+Enter
      3. Input is disabled, "Forge It!" shows loading animation
      4. SSE connection opens to POST /api/optimize
      5. Step 1 card activates: neon pulse, timer starts, content streams
      6. Step 1 completes: card collapses with checkmark, analysis badges appear
      7. Step 2 card activates: optimized prompt text streams in character by character
      8. Step 2 completes: card collapses, Step 3 activates
      9. Step 3 completes: all steps show checkmarks
      10. Result area fades in: diff view (default: side-by-side), scores animate in
      11. Input re-enables, "Forge It!" returns to normal
      12. History sidebar updates with new entry at top
    </optimization_flow>

    <history_interaction>
      1. Click history entry in sidebar
      2. Main area loads the full optimization record
      3. Pipeline steps show completed state with stored results
      4. Diff view and scores display from stored data
      5. "Re-forge" button available to re-run
    </history_interaction>

    <error_handling>
      - If claude-agent-sdk fails (CLI not found, auth error, rate limit):
        Show error in the active pipeline step card with neon-red border
        Display human-readable error message
        Offer "Retry" button
      - If a step produces invalid JSON:
        Fall back to displaying raw text output
        Mark step as warning (yellow) not failure
      - If validation step says optimization is NOT an improvement:
        Still show the result but with a yellow warning banner
        Offer "Re-forge with different approach" option
      - Network errors: toast notification with retry option
    </error_handling>
  </key_interactions>

  <project_structure>
    promptforge/
    ├── docker-compose.yml
    ├── .env.example
    ├── README.md
    ├── app_spec.txt
    │
    ├── backend/
    │   ├── Dockerfile
    │   ├── pyproject.toml
    │   ├── requirements.txt
    │   │
    │   └── app/
    │       ├── __init__.py
    │       ├── main.py                    # FastAPI app, CORS, lifespan, mount routes
    │       ├── config.py                  # Settings via pydantic-settings
    │       ├── database.py                # SQLAlchemy async engine, session, init
    │       │
    │       ├── models/
    │       │   ├── __init__.py
    │       │   └── optimization.py        # SQLAlchemy ORM model for optimizations table
    │       │
    │       ├── schemas/
    │       │   ├── __init__.py
    │       │   └── optimization.py        # Pydantic request/response schemas
    │       │
    │       ├── routers/
    │       │   ├── __init__.py
    │       │   ├── optimize.py            # POST /api/optimize, GET /api/optimize/{id}, POST retry
    │       │   ├── history.py             # GET /api/history, DELETE, GET stats
    │       │   └── health.py              # GET /api/health
    │       │
    │       ├── services/
    │       │   ├── __init__.py
    │       │   ├── pipeline.py            # Orchestrates the 3-step pipeline
    │       │   ├── analyzer.py            # Step 1: Prompt analysis
    │       │   ├── optimizer.py           # Step 2: Prompt optimization
    │       │   ├── validator.py           # Step 3: Result validation
    │       │   ├── claude_client.py       # Wrapper around claude-agent-sdk
    │       │   └── strategy_selector.py   # Maps analysis results to optimization strategy
    │       │
    │       ├── prompts/
    │       │   ├── __init__.py
    │       │   ├── analyzer_prompt.py     # System prompt for Step 1
    │       │   ├── optimizer_prompts.py   # Dynamic system prompts for Step 2 (per task_type)
    │       │   └── validator_prompt.py    # System prompt for Step 3
    │       │
    │       └── mcp_server.py              # MCP server entry point (FastMCP, stdio transport)
    │                                      # Reuses services/ and models/ — shares same DB
    │
    ├── frontend/
    │   ├── Dockerfile
    │   ├── package.json
    │   ├── svelte.config.js
    │   ├── vite.config.ts                 # Includes @tailwindcss/vite plugin
    │   ├── app.html
    │   ├── app.css                        # Global styles, @import "tailwindcss", @theme config, cyberpunk theme, fonts
    │   │
    │   └── src/
    │       ├── routes/
    │       │   ├── +layout.svelte         # Root layout with sidebar + main area
    │       │   └── +page.svelte           # Main page (input + pipeline + results)
    │       │
    │       └── lib/
    │           ├── components/
    │           │   ├── PromptInput.svelte          # Textarea + submit button
    │           │   ├── PipelineProgress.svelte     # 3-step progress display
    │           │   ├── PipelineStep.svelte         # Individual step card
    │           │   ├── ResultPanel.svelte           # Full result area
    │           │   ├── DiffView.svelte             # Toggleable diff display
    │           │   ├── SideBySideDiff.svelte       # Side-by-side diff renderer
    │           │   ├── InlineDiff.svelte           # Inline diff renderer
    │           │   ├── ScorePanel.svelte           # Score bars visualization
    │           │   ├── HistorySidebar.svelte       # Sidebar with history list
    │           │   ├── HistoryEntry.svelte         # Individual history item
    │           │   ├── Header.svelte               # Top bar with logo + stats
    │           │   ├── CopyButton.svelte           # Copy to clipboard with flash
    │           │   └── Toast.svelte                # Toast notifications
    │           │
    │           ├── stores/
    │           │   ├── optimization.ts     # Current optimization state ($state)
    │           │   └── history.ts          # History list state ($state)
    │           │
    │           ├── api/
    │           │   └── client.ts           # API client (fetch + SSE helpers)
    │           │
    │           └── utils/
    │               ├── diff.ts             # Diff computation wrapper
    │               ├── format.ts           # Date formatting, text truncation
    │               └── clipboard.ts        # Clipboard API wrapper
    │
    └── scripts/
        ├── dev.sh                          # Start both frontend + backend in dev mode
        └── seed_examples.py                # Optional: seed DB with example optimizations
  </project_structure>

  <implementation_steps>
    <step_1>
      TITLE: Project Foundation
      - Initialize project directories (backend/, frontend/, scripts/)
      - Set up Python project with pyproject.toml and requirements.txt
      - Set up SvelteKit 2 project with Svelte 5, Vite 7.3+, Tailwind CSS 4 (@tailwindcss/vite plugin)
      - Configure docker-compose.yml with backend + frontend services
      - Create .env.example with configuration variables
      - Set up FastAPI app skeleton with CORS, lifespan events
      - Set up SQLite database with SQLAlchemy async
      - Create the optimizations table schema
      - Verify both services start and communicate
    </step_1>

    <step_2>
      TITLE: Claude Client Service
      - Implement claude_client.py wrapper around claude-agent-sdk
      - Support both query() (stateless) and streaming modes
      - Implement structured JSON output parsing
      - Add error handling for CLI not found, auth errors, rate limits
      - Add retry logic with exponential backoff
      - Test basic prompt/response round-trip
    </step_2>

    <step_3>
      TITLE: Optimization Pipeline - Analysis
      - Write the analyzer system prompt (prompts/analyzer_prompt.py)
      - Implement analyzer.py service using claude_client
      - Define Pydantic schema for analysis output
      - Implement strategy_selector.py (task_type -> frameworks mapping)
      - Test analysis on diverse prompt types
      - Wire up to SSE streaming
    </step_3>

    <step_4>
      TITLE: Optimization Pipeline - Optimizer
      - Write dynamic optimizer system prompts for each task_type
      - Implement optimizer.py service
      - Implement framework injection logic (CO-STAR, RISEN, etc.)
      - Define Pydantic schema for optimization output
      - Test optimization across all task types
      - Wire up streaming (partial message forwarding via SSE)
    </step_4>

    <step_5>
      TITLE: Optimization Pipeline - Validator + Orchestration
      - Write validator system prompt
      - Implement validator.py service
      - Define Pydantic schema for validation output
      - Implement pipeline.py orchestrator (chains all 3 steps)
      - Add retry logic for failed validations (max 1 retry)
      - Implement SSE event emission for all step transitions
      - Create POST /api/optimize endpoint
      - End-to-end test of full pipeline
    </step_5>

    <step_6>
      TITLE: History & Persistence
      - Implement optimization model (SQLAlchemy ORM)
      - Save pipeline results to SQLite after completion
      - Implement GET /api/history with pagination, search, sort
      - Implement GET /api/optimize/{id} for single record retrieval
      - Implement DELETE /api/history/{id}
      - Implement GET /api/history/stats
      - Implement POST /api/optimize/{id}/retry
    </step_6>

    <step_7>
      TITLE: Frontend - Core Layout & Theme
      - Set up cyberpunk theme in app.css using Tailwind CSS 4 @theme directive
      - Import fonts (JetBrains Mono, Inter)
      - Build root layout (+layout.svelte) with sidebar + main structure
      - Build Header component with logo and stats
      - Build HistorySidebar component
      - Build PromptInput component with textarea and submit button
      - Implement keyboard shortcuts (Ctrl+Enter)
      - Test responsive layout (sidebar collapse on mobile)
    </step_7>

    <step_8>
      TITLE: Frontend - Pipeline & Streaming
      - Build SSE client in api/client.ts
      - Build PipelineProgress and PipelineStep components
      - Implement real-time streaming display (character-by-character text appearance)
      - Build step status indicators (active pulse, complete checkmark, pending circle)
      - Implement auto-collapse/expand behavior
      - Build duration timer per step
      - Connect to POST /api/optimize SSE endpoint
      - Test full streaming flow end-to-end
    </step_8>

    <step_9>
      TITLE: Frontend - Results & Diff
      - Implement diff computation (utils/diff.ts using `diff` npm package)
      - Build SideBySideDiff component with synchronized scrolling
      - Build InlineDiff component with word-level highlighting
      - Build DiffView component with toggle switch
      - Build ScorePanel with animated neon bars
      - Build ResultPanel composing all result sub-components
      - Build CopyButton with flash animation
      - Implement export to .md file download
      - Build changes_made summary list
    </step_9>

    <step_10>
      TITLE: Frontend - History Integration
      - Connect sidebar to GET /api/history
      - Implement history entry click -> load full optimization
      - Implement search/filter in sidebar
      - Implement delete with confirmation
      - Implement "Re-forge" and "Edit & Re-forge" flows
      - Build Toast component for notifications
    </step_10>

    <step_11>
      TITLE: MCP Server
      - Implement mcp_server.py using FastMCP with stdio transport
      - Register all 8 tools (optimize, get, list, get_by_project, search, tag, stats, delete)
      - Define Pydantic input models for each tool with Field constraints and descriptions
      - Connect to shared SQLite database (same DB as FastAPI backend)
      - Reuse pipeline services for the promptforge_optimize tool
      - Implement proper pagination for list/search tools
      - Add comprehensive docstrings with input/output schemas for each tool
      - Set correct tool annotations (readOnlyHint, destructiveHint, etc.)
      - Support both JSON and markdown response formats where applicable
      - Test via MCP Inspector: python -m app.mcp_server
      - Create example .mcp.json config for Claude Code integration
      - Document setup instructions for adding to Claude Code config
    </step_11>

    <step_12>
      TITLE: Polish & Production
      - Implement all animations (neon pulse, text stream, score bars, copy flash)
      - Add loading states and skeleton screens
      - Error handling UX (error cards, retry buttons, toast notifications)
      - Health check endpoint + frontend health indicator
      - Docker multi-stage builds for both services
      - Test full docker-compose deployment
      - Performance optimization (lazy loading, debouncing)
      - Accessibility basics (focus management, ARIA labels, keyboard navigation)
      - Verify MCP server works from Claude Code with docker-compose deployment
    </step_12>
  </implementation_steps>

  <success_criteria>
    <functionality>
      - User can paste any prompt and receive an optimized version in under 30 seconds
      - Pipeline correctly classifies at least 8 out of 10 prompt task types
      - Optimization produces genuinely improved prompts (validation score >= 7/10 on average)
      - Streaming shows real-time progress for each pipeline step
      - History persists across app restarts (Docker volume)
      - Diff view accurately highlights all changes between original and optimized
      - Copy to clipboard works reliably
      - Zero API costs — all LLM calls use subscription via claude-agent-sdk
      - MCP server accessible from Claude Code sessions for prompt optimization and retrieval
      - Prompts are organizable by project and tags, retrievable by any combination
      - MCP tools return well-structured JSON with clear schemas
    </functionality>

    <user_experience>
      - Page loads in under 1 second (Svelte 5 tiny bundle)
      - Cyberpunk theme is visually cohesive and distinctive
      - Streaming text appears smoothly without jank
      - All interactive elements have visible hover/focus states
      - Sidebar is usable for browsing 100+ history entries
      - Works on desktop (1200px+) and tablet (768px+)
    </user_experience>

    <technical_quality>
      - Clean separation of concerns (services, routers, schemas, components)
      - All backend endpoints have Pydantic validation
      - Frontend uses Svelte 5 runes consistently (no legacy reactive syntax)
      - Docker containers build and start with a single `docker-compose up`
      - No hardcoded secrets or API keys in source
      - Graceful error handling at every layer
      - SQLite database auto-creates on first run
      - MCP server reuses backend services (no code duplication)
      - MCP tools have proper Pydantic validation, annotations, and comprehensive docstrings
      - MCP server runs via stdio transport, configurable in Claude Code settings
    </technical_quality>
  </success_criteria>
</project_specification>
